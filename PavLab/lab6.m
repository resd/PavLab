clc;
clear all;
%настройки программы
N=20;      %количество объектов в обучающей выборке
s=20;       %количество циклов обучени€

%настройки алгоритма
err1 = 0;   %ошибка первого рода
err2 = 0;   %ошибка вторгог рода
P = 0;      %вероатность правильного распознаваниа

%инициализациа вектора мат. ожиданий и дисперсий признаков в классах
%согласно варианту заданиа
class1M = [1.2, 2.1];
class1D = [1.1, 1.3];
class2M = [5.8, 8.5];
class2D = [1.1, 1.3];

%формирование матрицы L х n случайных чисел с нормальным законом распределени€
class1(:,1) = normrnd(class1M(1), class1D(1), N, 1);
class1(:,2) = normrnd(class1M(2), class1D(2), N, 1);

class2(:,1) = normrnd(class2M(1), class2D(1), N, 1);
class2(:,2) = normrnd(class2M(2), class2D(2), N, 1);

%todo
%отображение элементов статистической выборки в пространстве признаков X1, X2
%figure(12);plot(class1(:,1),class1(:,2),'ro',class2(:,1),class2(:,2),'b+');hold on;

y(1:N)=0;
y(N+1:2*N)=1;

X=[class1; class2];

X = X'
y

%создание нейронной сети
%minmax(X) - матрица размерностью nx2 минимальных и максимальных значений вектора X
%[3,2] - количество нейронов в первом и втором слоах
%{'tansig','purelin'} - активационные функции слоев
net=newff(minmax(X),[20,1],{'tansig','purelin'});
%net=newff(minmax(X),[20,1],{'tansig','tansig','purelin'},'traingd');
%Ќастройка обучениа сети
net.trainParam.show = 20;       %врема обновлениа графика обучениа сети
net.trainParam.lr = 0.01;       
net.trainParam.epochs = 100;    %врема обучениа сети
net.trainParam.goal = 5e-2;     %точность обучениа сети
%обучение сети обратного распространениа
[net,tr]=train(net,X,y);
%определение ошибок 1 и 2 рода и вероатности правильного распознаваниа
s = sim(net,X);
s1 = s(1,1:2*N);
s1
err1=0;
err2=0;
for i=1:1:(2*N)
    if (i<(N+1))&(s1(i)>0.5)
        err1=err1+1;
    end
    if (i>N)&(s1(i)<.5)
        err2=err2+1;
    end
end
tppr = (1 - (err1 + err2) / (N + N)) * 100.0;
err1
err2
tppr